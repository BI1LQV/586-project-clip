# CLIP Project: Exploring Vision-Language Alignment

This project explores the capabilities and limitations of CLIP, a vision-language model developed by OpenAI that maps images and texts into a shared embedding space. Our goal is to understand how well CLIP performs across several tasks, including image classification, image-text retrieval, and robustness evaluation under text perturbations.


## ğŸ“ Project Structure
```
586-PROJECT-CLIP/
â”œâ”€â”€ captions/                   # Modified captions for robustness testing
â”‚   â”œâ”€â”€ all_captions.txt
â”‚   â”œâ”€â”€ kana.txt
â”‚   â”œâ”€â”€ roman.txt
â”‚   â”œâ”€â”€ same_meaning_replace.txt
â”‚   â””â”€â”€ sov.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ food-101.tar.gz         # Raw dataset used in some experiments
â”œâ”€â”€ clip.ipynb                  # Main notebook running all experiments
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md                   # Project description (you are here!)
```
## ğŸ“Œ Objectives

- Evaluate CLIP's ability in zero-shot image-text matching scenarios.
- Test CLIP's robustness against textual modifications.

## ğŸ“Š Key Findings

- CLIP demonstrates near-perfect adaptability to pure English. Even when the sentences are altered through inflectional changes or word order disruptions, it still exhibits an extremely excellent level of understanding.
- However, once the captions are translated into foreign languages, CLIP's performance deteriorates drastically.

## ğŸ“ Dataset

We use:
- [Flickr8k](https://forms.illinois.edu/sec/1713398)
- [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)

## ğŸš€ Getting Started

```bash
# Clone the repo
git clone https://github.com/yourusername/586-PROJECT-CLIP.git
cd 586-PROJECT-CLIP

# Open the notebook
jupyter notebook clip.ipynb

```


## ğŸ“„ License
This project is licensed under the terms of the LICENSE file.
